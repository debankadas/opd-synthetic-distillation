########################################
# KD/Training (On-policy Distillation)
########################################
#STUDENT_MODEL_PATH=<add local model path if custom model is used>
# Default student model (qwen 0.5B) - override as needed
STUDENT_MODEL_ID=Qwen/Qwen2.5-0.5B-Instruct

# Synthetic Data OPD Training
DATA_PATH=synthetic_datasets/combined_100.json
OUTPUT_DIR=outputs/Qwen2.5-0.5b-Instruct-OPD
MAX_STEPS=300
SAVE_EVERY=100
LEARNING_RATE=5e-5
MAX_NEW_TOKENS=1200  # Dataset avg=533, max=1052 tokens adjust as per dataset and memory availablity
BETA=0.0
NUM_SAMPLES=all # or use 10/100/1000 for smaller subsets during testing

# Optional: Enable LoRA for memory efficiency
KD_LORA=1
KD_LORA_R=16
KD_LORA_ALPHA=32


########################################
# Teacher Backend Configuration
########################################
# Choose backend: "openrouter" (API), "gguf" (local quantized), or "hf" (local full model)
# TEACHER_BACKEND=openrouter   # Lightweight API-based (recommended)
# TEACHER_BACKEND=hf         # Local quantized GGUF model
TEACHER_BACKEND=gguf             # Local full HuggingFace model (heavy)

# OpenRouter Settings (when TEACHER_BACKEND=openrouter)
# Uncomment the line below to use your OpenRouter API key
OPENROUTER_API_KEY=
OPENROUTER_MODEL=qwen/qwen-2.5-7b-instruct
# Alternative models you can try:
# OPENROUTER_MODEL=meta-llama/llama-3.1-8b-instruct
# OPENROUTER_MODEL=google/gemini-pro-1.5

# Teacher GGUF (when TEACHER_BACKEND=gguf) — local .gguf or HF repo id/URL
# Use local file path to avoid network checks (recommended if model already downloaded)
TEACHER_GGUF_PATH=models/gguf/bartowski_Qwen2.5-7B-Instruct-GGUF/Qwen2.5-7B-Instruct-Q4_K_M.gguf
# Fallback to repo ID if local file not found
TEACHER_GGUF=bartowski/Qwen2.5-7B-Instruct-GGUF
TEACHER_PREFERRED_QUANT=Q4_K_M
TEACHER_N_GPU_LAYERS=-1
TEACHER_N_CTX=2048

# Teacher HuggingFace (when TEACHER_BACKEND=hf) — full precision local model
TEACHER_MODEL_ID=Qwen/Qwen2.5-7B-Instruct
TEACHER_DTYPE=bf16
TEACHER_FLASH_ATTENTION=1

# Shared teacher settings
TEACHER_TOP_K=50

# KD/SFT blend and gating
LAMBDA_SFT=0.3
LAMBDA_KD=0.7
KD_MIN_TEACHER_LOGP=-30
KD_DEBUG=0
KD_LORA=1
KD_LORA_R=16
KD_LORA_ALPHA=32
KD_LORA_DROPOUT=0.05
KD_LORA_TARGET=q_proj,k_proj,v_proj,o_proj
KD_GRAD_CLIP=1.0
KD_MAX_NEW_TOKENS=1200  # Dataset avg=533, max=1052 tokens (helps on MPS)
KD_MAX_GROUPS=128  # uncomment to cap KD backprop groups (helps on MPS)

# Validation / early stopping
VAL_FRACTION=0.1
EVAL_EVERY=50
PATIENCE=3
MONITOR_METRIC=val_top1_agreement
MONITOR_MODE=max
VAL_MAX_SAMPLES=200
MAX_STEPS=500

# Output directory for KD (Mac)
OUTPUT_DIR=outputs/Qwen2.5-0.5b-Instruct-OPD

########################################
# Misc / optional
########################################
# Prefer Xet-backed HF downloads (optional)
HF_BACKEND=xet
HUGGINGFACE_HUB_TOKEN=

# Weights & Biases (optional). Set to enable logging.
WANDB_API_KEY=
WANDB_ENTITY=

OPENAI_API_KEY=